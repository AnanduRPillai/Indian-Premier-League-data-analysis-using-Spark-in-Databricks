# Indian-Premier-League-data-analysis-using-Spark-and-AWS s3-in-Databricks



## Project Description

In this project, we are going to analyze IPL data stored in an S3 bucket by building a data pipeline. The main focus is on writing Apache Spark code and implementing different functions to perform transformations in Databricks.


## Installation Instructions

To run this project, you need the following:

-Databricks account

-Databricks CLI installed and configured on your local machine

-Python 3.x

-AWS S3 bucket

## Usage Instructions
Step 1: Set Up Databricks CLI

Step 2: Create a Databricks Cluster. Log in to your Databricks workspace and create a new cluster with appropriate configurations (e.g., number of nodes, instance types). Note the Cluster ID for later use.

Step 3: Install Necessary Libraries.

Step 4: Navigate to the Workspace and Run Notebooks. Navigate to the created workspace in your Databricks workspace and open and run the notebooks.

## Data

Data Description: Data including Ball By Ball, Total Matches, Total Players,  Players in each Match , Teams at all the IPL seasons (637 matches including 2017) A Complete datawarehouse

Data: https://data.world/raghu543/ipl-data-till-2017


